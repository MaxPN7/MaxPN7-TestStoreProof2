---
title: "introduction"
bg: blue
color: white
fa-icon: quote-left
---

Audio and visual modalities are the most commonly used by humans to identify other humans and sense their emotional state.
Features extracted from these two modalities are often highly correlated, providing us with the capability of imagining 
the visual appearance of a person just by listening to his voice or building some expectations about the tone or pitch of her voice from a picture.

We present Speech2YouTuber, a method that aims at imagining an image of a face that could correspond to a provided speech utterance.
Our solution is based on recent advances on deep generative models, namely Variational Auto-Encoders (VAE)  and Generative Adversarial Networks (GAN).
Speech2YouTuber is inspired on previous works that have conditioned the generation of images using text or audio features. In this work, we condition 
the generative process with raw speech.

If you find this work useful, please consider citing us:


Download our paper in pdf [here]().
